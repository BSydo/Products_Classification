{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Products classification\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import all libs we will use for that task\n",
    "\n",
    "[sklearn](https://scikit-learn.org/stable/) \n",
    "[pandas](https://pandas.pydata.org/)\n",
    "[xgboost](https://xgboost.readthedocs.io/en/latest/)\n",
    "[numpy](https://www.numpy.org/)\n",
    "[textblob](https://textblob.readthedocs.io/en/dev/)\n",
    "[string](http://effbot.org/librarybook/string.htm)\n",
    "[keras](https://keras.io/)\n",
    "[sys](https://docs.python.org/3/library/sys.html)\n",
    "[wget](https://pypi.org/project/wget/)\n",
    "[zipfile](https://docs.python.org/3/library/zipfile.html)\n",
    "[os](https://docs.python.org/3/library/os.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional: \n",
    "\n",
    "- set __location__ as current location of project for easy access the files and ignore warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import xgboost, textblob, string, sys, wget, zipfile, os\n",
    "from keras import layers, models, optimizers\n",
    "from sklearn import decomposition, ensemble\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "__location__ = sys.path[0]\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load\n",
    "- Load the dataset\n",
    "- Load the pre-trained model for word-embedding and place it in the project folder\n",
    "- Make the combined feature from given data\n",
    "- Define __Text__ and __Label__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-trained word embeddings (2.2 GB) now will be loaded, please wait.\n",
      "complete\n",
      "extracting\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "data = pd.read_excel('{}/Products.xlsx'.format(__location__), index_col=0,header=0)\n",
    "\n",
    "#load pre-trained word emmbeddings\n",
    "# downloading ~2,2 Gb datasets \n",
    "url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "wget.download(url, '{}/glove.6B.zip'.format(__location__))\n",
    "\n",
    "#Extract datasets to project folder\n",
    "with zipfile.ZipFile('{}/glove.6B.zip'.format(__location__), 'r') as zip_ref:\n",
    "    zip_ref.extractall('{}/'.format(__location__))\n",
    "\n",
    "#combine with product name and category\n",
    "data['f_name'] = data[['description','initcat','case_size','pack_desc',]].apply(lambda x: ' '.join(str(x)),axis=1)\n",
    "trainDF = pd.DataFrame()\n",
    "\n",
    "#define text and label\n",
    "trainDF['text'] = data.f_name\n",
    "trainDF['label'] = data.our_category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split the dataset to __train__ and __test__. By default it is 80/20 and we won't change it\n",
    "\n",
    "- Make a label encoding for train and valid datasets. Simply encode your labels by using __preprocessing__ package. This will also allow you to decode them later with .inverse_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])\n",
    "\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer object\n",
    "#### Convert a collection of text documents to a matrix of token counts\n",
    "\n",
    "If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the number of features will be equal to the vocabulary size found by analyzing the data.\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "__input : string {'filename', 'file', 'content'}__\n",
    "If 'filename', the sequence passed as an argument to fit is expected to be a list of filenames that need reading to fetch the raw content to analyze.\n",
    "\n",
    "If 'file', the sequence items must have a 'read' method (file-like object) that is called to fetch the bytes in memory.\n",
    "\n",
    "Otherwise the input is expected to be the sequence strings or bytes items are expected to be analyzed directly.\n",
    "\n",
    "__encoding : string, 'utf-8' by default.__\n",
    "\n",
    "If bytes or files are given to analyze, this encoding is used to decode.\n",
    "\n",
    "__decode_error : {'strict', 'ignore', 'replace'}__\n",
    "\n",
    "Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given encoding. By default, it is 'strict', meaning that a UnicodeDecodeError will be raised. Other values are 'ignore' and 'replace'.\n",
    "\n",
    "__strip_accents : {'ascii', 'unicode', None}__\n",
    "\n",
    "Remove accents during the preprocessing step. 'ascii' is a fast method that only works on characters that have an direct ASCII mapping. 'unicode' is a slightly slower method that works on any characters. None (default) does nothing.\n",
    "\n",
    "__analyzer : string, {'word', 'char', 'char_wb'} or callable__\n",
    "\n",
    "Whether the feature should be made of word or character n-grams. Option 'char_wb' creates character n-grams only from text inside word boundaries.\n",
    "\n",
    "If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input. Only applies if analyzer == 'word'.\n",
    "\n",
    "__preprocessor : callable or None (default)__\n",
    "\n",
    "Override the preprocessing (string transformation) stage while preserving the tokenizing and n-grams generation steps.\n",
    "\n",
    "__tokenizer : callable or None (default)__\n",
    "\n",
    "Override the string tokenization step while preserving the preprocessing and n-grams generation steps. Only applies if analyzer == 'word'.\n",
    "\n",
    "__ngram_range : tuple (min_n, max_n)__\n",
    "\n",
    "The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such \n",
    "that min_n <= n <= max_n will be used.\n",
    "\n",
    "__stop_words : string {'english'}, list, or None (default)__\n",
    "\n",
    "If 'english', a built-in stop word list for English is used.\n",
    "\n",
    "If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if analyzer == 'word'.\n",
    "\n",
    "If None, no stop words will be used. max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms.\n",
    "\n",
    "__lowercase : boolean, True by default__\n",
    "\n",
    "Convert all characters to lowercase before tokenizing.\n",
    "\n",
    "__token_pattern : string__\n",
    "\n",
    "Regular expression denoting what constitutes a \"token\", only used if analyzer == 'word'. The default regexp select tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator).\n",
    "\n",
    "__max_df : float in range [0.0, 1.0] or int, default=1.0__\n",
    "\n",
    "When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "__min_df : float in range [0.0, 1.0] or int, default=1__\n",
    "\n",
    "When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "__max_features : int or None, default=None__\n",
    "\n",
    "If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "\n",
    "This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "__vocabulary : Mapping or iterable, optional__\n",
    "\n",
    "Either a Mapping (e.g., a dict) where keys are terms and values are indices in the feature matrix, or an iterable over terms. If not given, a vocabulary is determined from the input documents. Indices in the mapping should not be repeated and should not have any gap between 0 and the largest index.\n",
    "\n",
    "__binary : boolean, default=False__\n",
    "\n",
    "If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts.\n",
    "\n",
    "__dtype : type, optional__\n",
    "\n",
    "Type of the matrix returned by fit_transform() or transform().\n",
    "\n",
    "#### Attributes\n",
    "\n",
    "__vocabulary_ : dict__\n",
    "\n",
    "A mapping of terms to feature indices.\n",
    "\n",
    "__stop_words_ : set__\n",
    "\n",
    "Terms that were ignored because they either:\n",
    "\n",
    "- occurred in too many documents (max_df)\n",
    "\n",
    "- occurred in too few documents (min_df)\n",
    "\n",
    "were cut off by feature selection (max_features).\n",
    "This is only available if no vocabulary was given.\n",
    "\n",
    "- Notes\n",
    "The stop_words_ attribute can get large and increase the model size when pickling. This attribute is provided only for introspection and can be safely removed using delattr or set to None before pickling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(trainDF['text'])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "__TF-IDF__ stands for “Term Frequency — Inverse Data Frequency”. What this term means mathematically?\n",
    "\n",
    "__Term Frequency (tf)__: gives us the frequency of the word in each document in the corpus. It is the ratio of number of times the word appears in a document compared to the total number of words in that document. It increases as the number of occurrences of that word within the document increases. Each document has its own tf.\n",
    "\n",
    "![image.png](https://cdn-media-1.freecodecamp.org/images/1*HM0Vcdrx2RApOyjp_ZeW_Q.png)\n",
    "\n",
    "__Inverse Data Frequency (idf)__: used to calculate the weight of rare words across all documents in the corpus. The words that occur rarely in the corpus have a high IDF score. It is given by the equation below.\n",
    "\n",
    "![image.png](https://cdn-media-1.freecodecamp.org/images/1*A5YGwFpcTd0YTCdgoiHFUw.png)\n",
    "\n",
    "Combining these two we come up with the __TF-IDF score (w)__ for a word in a document in the corpus. It is the product of tf and idf:\n",
    "\n",
    "![image.png](https://cdn-media-1.freecodecamp.org/images/1*nSqHXwOIJ2fa_EFLTh5KYw.png)\n",
    "\n",
    "![image.png](https://cdn-media-1.freecodecamp.org/images/1*q2tRgjV_J-MLvnhwNAl0KQ.png)\n",
    "\n",
    "### N-Grams Model\n",
    "\n",
    "Wikipedia defines an N-Gram as \"A contiguous sequence of N items from a given sample of text or speech\". Here an item can be a character, a word or a sentence and N can be any integer. When N is 2, we call the sequence a bigram. Similarly, a sequence of 3 items is called a trigram, and so on.\n",
    "\n",
    "__In order to understand N-Grams model, we first have to understand how the Markov chains work.__\n",
    "\n",
    "_Connection of N-Grams with Markov Chains_\n",
    "\n",
    "A Markov chain is a sequence of states. Consider a Markov system with 2 states, X and Y. In a Markov chain, you can either stay at one state or move to the other state. In our example, our states have the following behavior:\n",
    "\n",
    "- The probability of moving from X to Y is 50% and similarly, the probability of staying at X is 50%.Likewise, the probability of staying at Y is 50% while the possibility of moving back to X is also 50%. This way a Markov sequence can be generated, such as XXYX, etc.\n",
    "\n",
    "\n",
    "- In an N-Grams model, an item in a sequence can be treated as a Markov state. Let's see a simple example of character bigrams where each character is a Markov state.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"><b>Football is a very famous game</b></div>\n",
    "\n",
    "The character bigrams for the above sentence will be: __fo__, __oo__, __ot__, __tb__, __ba__, __al__, __ll__, __l___, ___i__, __is__ and so on. You can see that bigrams are basically a sequence of two consecutively occurring characters.\n",
    "\n",
    "Similarly, the __trigrams__ are a sequence of three contiguous characters, as shown below:\n",
    "\n",
    "__foo__, __oot__, __otb__, __tba__ and so on.\n",
    "\n",
    "### Implementation - Python\n",
    "\n",
    "__Convert a collection of raw documents to a matrix of TF-IDF features.__\n",
    "\n",
    "Equivalent to CountVectorizer followed by TfidfTransformer.\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "__input : string {'filename', 'file', 'content'}__\n",
    "\n",
    "If 'filename', the sequence passed as an argument to fit is expected to be a list of filenames that need reading to fetch the raw content to analyze.\n",
    "\n",
    "If 'file', the sequence items must have a 'read' method (file-like object) that is called to fetch the bytes in memory.\n",
    "\n",
    "Otherwise the input is expected to be the sequence strings or bytes items are expected to be analyzed directly.\n",
    "\n",
    "__encoding : string, 'utf-8' by default.__\n",
    "\n",
    "If bytes or files are given to analyze, this encoding is used to decode.\n",
    "\n",
    "__decode_error : {'strict', 'ignore', 'replace'}__\n",
    "\n",
    "Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given encoding. By default, it is 'strict', meaning that a UnicodeDecodeError will be raised. Other values are 'ignore' and 'replace'.\n",
    "\n",
    "__strip_accents : {'ascii', 'unicode', None}__\n",
    "\n",
    "Remove accents during the preprocessing step. 'ascii' is a fast method that only works on characters that have an direct ASCII mapping. 'unicode' is a slightly slower method that works on any characters. None (default) does nothing.\n",
    "\n",
    "__analyzer : string, {'word', 'char'} or callable__\n",
    "Whether the feature should be made of word or character n-grams.\n",
    "\n",
    "If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input.\n",
    "\n",
    "__preprocessor : callable or None (default)__\n",
    "\n",
    "Override the preprocessing (string transformation) stage while preserving the tokenizing and n-grams generation steps.\n",
    "\n",
    "__tokenizer : callable or None (default)__\n",
    "\n",
    "Override the string tokenization step while preserving the preprocessing and n-grams generation steps. Only applies if analyzer == 'word'.\n",
    "\n",
    "__ngram_range : tuple (min_n, max_n)__\n",
    "\n",
    "The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used.\n",
    "\n",
    "__stop_words : string {'english'}, list, or None (default)__\n",
    "\n",
    "If a string, it is passed to _check_stop_list and the appropriate stop list is returned. 'english' is currently the only supported string value.\n",
    "\n",
    "If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if analyzer == 'word'.\n",
    "\n",
    "If None, no stop words will be used. max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms.\n",
    "\n",
    "__lowercase : boolean, default True__\n",
    "\n",
    "Convert all characters to lowercase before tokenizing.\n",
    "\n",
    "__token_pattern : string__\n",
    "\n",
    "Regular expression denoting what constitutes a \"token\", only used if analyzer == 'word'. The default regexp selects tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator).\n",
    "\n",
    "__max_df : float in range [0.0, 1.0] or int, default=1.0__\n",
    "\n",
    "When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "__min_df : float in range [0.0, 1.0] or int, default=1__\n",
    "\n",
    "When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "__max_features : int or None, default=None__\n",
    "\n",
    "If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "\n",
    "This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "__vocabulary : Mapping or iterable, optional__\n",
    "\n",
    "Either a Mapping (e.g., a dict) where keys are terms and values are indices in the feature matrix, or an iterable over terms. If not given, a vocabulary is determined from the input documents.\n",
    "\n",
    "__binary : boolean, default=False__\n",
    "\n",
    "If True, all non-zero term counts are set to 1. This does not mean outputs will have only 0/1 values, only that the tf term in tf-idf is binary. (Set idf and normalization to False to get 0/1 outputs.)\n",
    "\n",
    "__dtype : type, optional__\n",
    "\n",
    "Type of the matrix returned by fit_transform() or transform().\n",
    "\n",
    "__norm : 'l1', 'l2' or None, optional__\n",
    "\n",
    "Norm used to normalize term vectors. None for no normalization.\n",
    "\n",
    "__use_idf : boolean, default=True__\n",
    "\n",
    "Enable inverse-document-frequency reweighting.\n",
    "\n",
    "__smooth_idf : boolean, default=True__\n",
    "\n",
    "Smooth idf weights by adding one to document frequencies, as if an extra document was seen containing every term in the collection exactly once. Prevents zero divisions.\n",
    "\n",
    "__sublinear_tf : boolean, default=False__\n",
    "\n",
    "Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
    "\n",
    "\n",
    "#### Attributes\n",
    "\n",
    "__idf_ : array, shape = [n_features], or None__\n",
    "\n",
    "The learned idf vector (global term weights) when use_idf is set to True, None otherwise.\n",
    "\n",
    "__stop_words_ : set__\n",
    "\n",
    "Terms that were ignored because they either:\n",
    "\n",
    "- occurred in too many documents (max_df)\n",
    "\n",
    "\n",
    "- occurred in too few documents (min_df)\n",
    "\n",
    "\n",
    "- were cut off by feature selection (max_features).\n",
    "\n",
    "This is only available if no vocabulary was given.\n",
    "\n",
    "#### Notes\n",
    "\n",
    "The __stop_words___ attribute can get large and increase the model size when pickling. This attribute is provided only for introspection and can be safely removed using delattr or set to None before pickling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(trainDF['text'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', \n",
    "                                   ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(trainDF['text'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', \n",
    "                                         ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(trainDF['text'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import text, sequence\n",
    "# load the pre-trained word-embedding vectors \n",
    "embeddings_index = {}\n",
    "for i, line in enumerate(open('{}/glove.6B.300d.txt'.format(__location__))):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\n",
    "\n",
    "# create a tokenizer \n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(trainDF['text'])\n",
    "word_index = token.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF['char_count'] = trainDF['text'].apply(len)\n",
    "trainDF['word_count'] = trainDF['text'].apply(lambda x: len(x.split()))\n",
    "trainDF['word_density'] = trainDF['char_count'] / (trainDF['word_count']+1)\n",
    "trainDF['punctuation_count'] = trainDF['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "trainDF['title_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "trainDF['upper_case_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_family = {'noun' : ['NN','NNS','NNP','NNPS'],\n",
    "              'pron' : ['PRP','PRP$','WP','WP$'],\n",
    "              'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
    "              'adj' :  ['JJ','JJR','JJS'],\n",
    "              'adv' : ['RB','RBR','RBS','WRB']\n",
    "             }\n",
    "\n",
    "# function to check and get the part of speech tag count of a words in a given sentence\n",
    "def check_pos_tag(x, flag):\n",
    "    cnt = 0\n",
    "    try:\n",
    "        wiki = textblob.TextBlob(x)\n",
    "        for tup in wiki.tags:\n",
    "            ppo = list(tup)[1]\n",
    "            if ppo in pos_family[flag]:\n",
    "                cnt += 1\n",
    "    except:\n",
    "        pass\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF['noun_count'] = trainDF.text.apply(lambda x: check_pos_tag(x, 'noun'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF['verb_count'] = trainDF.text.apply(lambda x: check_pos_tag(x, 'verb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF['adj_count'] = trainDF.text.apply(lambda x: check_pos_tag(x, 'adj'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF['adv_count'] = trainDF.text.apply(lambda x: check_pos_tag(x, 'adv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF['pron_count'] = trainDF.text.apply(lambda x: check_pos_tag(x, 'pron'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a LDA Model\n",
    "lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n",
    "X_topics = lda_model.fit_transform(xtrain_count)\n",
    "topic_word = lda_model.components_ \n",
    "vocab = count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the topic models\n",
    "n_top_words = 10\n",
    "topic_summaries = []\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    topic_summaries.append(' '.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors:  0.4135106529842161\n",
      "NB, WordLevel TF-IDF:  0.21351985642630344\n",
      "NB, N-Gram Vectors:  0.8511343242372648\n",
      "NB, CharLevel Vectors:  0.612535088122958\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Count Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "print (\"NB, Count Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"NB, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors ++++++++++++++++++++++++++++++++++++++++++\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print (\"NB, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print (\"NB, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors:  0.48999125673001703\n",
      "LR, WordLevel TF-IDF:  0.47089411439878515\n",
      "LR, N-Gram Vectors:  0.9180893654226681\n",
      "LR, CharLevel Vectors:  0.8446919147761263\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Count Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
    "print (\"LR, Count Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"LR, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors++++++++++++++++++++++++++++++++++++++++++\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print (\"LR, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Character Level TF IDF Vectors++++++++++++++++++++++++++++++++++++++++++\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, \n",
    "                       train_y, xvalid_tfidf_ngram_chars)\n",
    "print (\"LR, CharLevel Vectors: \", accuracy)\n",
    "\n",
    "#----------------------------------------------------------\n",
    "lgr = linear_model.LogisticRegression\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors -- lgfgs\n",
    "accuracy = train_model(lgr(solver='lbfgs', multi_class='multinomial')\n",
    "                       , xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print (\"LR, N-Gram Vectors, lbfgs: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors -- sag\n",
    "accuracy = train_model(lgr(solver='sag', multi_class='multinomial')\n",
    "                       , xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print (\"LR, N-Gram Vectors, sag: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors -- saga\n",
    "accuracy = train_model(lgr(solver='saga', multi_class='multinomial')\n",
    "                       , xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print (\"LR, N-Gram Vectors, saga: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors -- newton-cg\n",
    "accuracy = train_model(lgr(solver='newton-cg', multi_class='multinomial')\n",
    "                       , xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print (\"LR, N-Gram Vectors, newton-cg: \", accuracy)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# SVM on Ngram Level TF IDF Vectors\n",
    "#accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "#print (\"SVM, N-Gram Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, Count Vectors:  0.7435460862362524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, WordLevel TF-IDF:  0.7103676775113893\n"
     ]
    }
   ],
   "source": [
    "# RF on Count Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
    "print (\"RF, Count Vectors: \", accuracy)\n",
    "\n",
    "# RF on Word Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"RF, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, Count Vectors:  0.5702912889420644\n",
      "Xgb, WordLevel TF-IDF:  0.5831761078643413\n",
      "Xgb, CharLevel Vectors:  0.8656297455248263\n"
     ]
    }
   ],
   "source": [
    "# Extereme Gradient Boosting on Count Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
    "print (\"Xgb, Count Vectors: \", accuracy)\n",
    "\n",
    "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
    "print (\"Xgb, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Extereme Gradient Boosting on Character Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), train_y, \n",
    "                       xvalid_tfidf_ngram_chars.tocsc())\n",
    "print (\"Xgb, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "65190/65190 [==============================] - 14s 221us/step - loss: -139.2046\n",
      "NN, Ngram Level TF IDF Vectors 0.2128756154801896\n"
     ]
    }
   ],
   "source": [
    "def create_model_architecture(input_size):\n",
    "    # create input layer \n",
    "    input_layer = layers.Input((input_size, ), sparse=True)\n",
    "    \n",
    "    # create hidden layer\n",
    "    hidden_layer = layers.Dense(100, activation=\"relu\")(input_layer)\n",
    "    \n",
    "    # create output layer\n",
    "    output_layer = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
    "\n",
    "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
    "    classifier.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    return classifier \n",
    "\n",
    "classifier = create_model_architecture(xtrain_tfidf_ngram.shape[1])\n",
    "accuracy = train_model(classifier, xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, is_neural_net=True)\n",
    "print (\"NN, Ngram Level TF IDF Vectors\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0818 01:12:15.175818 140026582972160 deprecation.py:506] From /usr/local/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "65190/65190 [==============================] - 15s 229us/step - loss: -142.2116\n",
      "CNN, Word Embeddings 0.2128756154801896\n"
     ]
    }
   ],
   "source": [
    "def create_cnn():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], \n",
    "                                       trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_cnn()\n",
    "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print (\"CNN, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "65190/65190 [==============================] - 61s 937us/step - loss: -141.5640\n",
      "RNN-LSTM, Word Embeddings 0.2128756154801896\n"
     ]
    }
   ],
   "source": [
    "def create_rnn_lstm():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], \n",
    "                                       trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.LSTM(100)(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_rnn_lstm()\n",
    "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print (\"RNN-LSTM, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All additional files were removed\n"
     ]
    }
   ],
   "source": [
    "os.remove('{}/glove.6B.zip'.format(__location__))\n",
    "os.remove('{}/glove.6B.50d.txt'.format(__location__))\n",
    "os.remove('{}/glove.6B.100d.txt'.format(__location__))\n",
    "os.remove('{}/glove.6B.200d.txt'.format(__location__))\n",
    "os.remove('{}/glove.6B.300d.txt'.format(__location__))\n",
    "print(\"All additional files were removed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
